{"cells":[{"cell_type":"markdown","metadata":{},"source":["- https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-22T14:09:48.287926Z","iopub.status.busy":"2022-12-22T14:09:48.287368Z","iopub.status.idle":"2022-12-22T14:09:48.328186Z","shell.execute_reply":"2022-12-22T14:09:48.327275Z","shell.execute_reply.started":"2022-12-22T14:09:48.287815Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/otto-validation/test_labels.parquet\n","/kaggle/input/otto-validation/test_parquet/011.parquet\n","/kaggle/input/otto-validation/test_parquet/006.parquet\n","/kaggle/input/otto-validation/test_parquet/017.parquet\n","/kaggle/input/otto-validation/test_parquet/016.parquet\n","/kaggle/input/otto-validation/test_parquet/005.parquet\n","/kaggle/input/otto-validation/test_parquet/019.parquet\n","/kaggle/input/otto-validation/test_parquet/012.parquet\n","/kaggle/input/otto-validation/test_parquet/015.parquet\n","/kaggle/input/otto-validation/test_parquet/007.parquet\n","/kaggle/input/otto-validation/test_parquet/018.parquet\n","/kaggle/input/otto-validation/test_parquet/000.parquet\n","/kaggle/input/otto-validation/test_parquet/001.parquet\n","/kaggle/input/otto-validation/test_parquet/008.parquet\n","/kaggle/input/otto-validation/test_parquet/010.parquet\n","/kaggle/input/otto-validation/test_parquet/002.parquet\n","/kaggle/input/otto-validation/test_parquet/014.parquet\n","/kaggle/input/otto-validation/test_parquet/009.parquet\n","/kaggle/input/otto-validation/test_parquet/003.parquet\n","/kaggle/input/otto-validation/test_parquet/004.parquet\n","/kaggle/input/otto-validation/test_parquet/013.parquet\n","/kaggle/input/otto-validation/train_parquet/045.parquet\n","/kaggle/input/otto-validation/train_parquet/011.parquet\n","/kaggle/input/otto-validation/train_parquet/050.parquet\n","/kaggle/input/otto-validation/train_parquet/087.parquet\n","/kaggle/input/otto-validation/train_parquet/040.parquet\n","/kaggle/input/otto-validation/train_parquet/049.parquet\n","/kaggle/input/otto-validation/train_parquet/006.parquet\n","/kaggle/input/otto-validation/train_parquet/017.parquet\n","/kaggle/input/otto-validation/train_parquet/024.parquet\n","/kaggle/input/otto-validation/train_parquet/032.parquet\n","/kaggle/input/otto-validation/train_parquet/078.parquet\n","/kaggle/input/otto-validation/train_parquet/089.parquet\n","/kaggle/input/otto-validation/train_parquet/023.parquet\n","/kaggle/input/otto-validation/train_parquet/029.parquet\n","/kaggle/input/otto-validation/train_parquet/068.parquet\n","/kaggle/input/otto-validation/train_parquet/074.parquet\n","/kaggle/input/otto-validation/train_parquet/096.parquet\n","/kaggle/input/otto-validation/train_parquet/016.parquet\n","/kaggle/input/otto-validation/train_parquet/077.parquet\n","/kaggle/input/otto-validation/train_parquet/021.parquet\n","/kaggle/input/otto-validation/train_parquet/046.parquet\n","/kaggle/input/otto-validation/train_parquet/055.parquet\n","/kaggle/input/otto-validation/train_parquet/058.parquet\n","/kaggle/input/otto-validation/train_parquet/037.parquet\n","/kaggle/input/otto-validation/train_parquet/051.parquet\n","/kaggle/input/otto-validation/train_parquet/005.parquet\n","/kaggle/input/otto-validation/train_parquet/019.parquet\n","/kaggle/input/otto-validation/train_parquet/081.parquet\n","/kaggle/input/otto-validation/train_parquet/028.parquet\n","/kaggle/input/otto-validation/train_parquet/097.parquet\n","/kaggle/input/otto-validation/train_parquet/095.parquet\n","/kaggle/input/otto-validation/train_parquet/012.parquet\n","/kaggle/input/otto-validation/train_parquet/027.parquet\n","/kaggle/input/otto-validation/train_parquet/033.parquet\n","/kaggle/input/otto-validation/train_parquet/052.parquet\n","/kaggle/input/otto-validation/train_parquet/020.parquet\n","/kaggle/input/otto-validation/train_parquet/065.parquet\n","/kaggle/input/otto-validation/train_parquet/036.parquet\n","/kaggle/input/otto-validation/train_parquet/056.parquet\n","/kaggle/input/otto-validation/train_parquet/043.parquet\n","/kaggle/input/otto-validation/train_parquet/060.parquet\n","/kaggle/input/otto-validation/train_parquet/015.parquet\n","/kaggle/input/otto-validation/train_parquet/066.parquet\n","/kaggle/input/otto-validation/train_parquet/034.parquet\n","/kaggle/input/otto-validation/train_parquet/099.parquet\n","/kaggle/input/otto-validation/train_parquet/047.parquet\n","/kaggle/input/otto-validation/train_parquet/041.parquet\n","/kaggle/input/otto-validation/train_parquet/022.parquet\n","/kaggle/input/otto-validation/train_parquet/007.parquet\n","/kaggle/input/otto-validation/train_parquet/082.parquet\n","/kaggle/input/otto-validation/train_parquet/084.parquet\n","/kaggle/input/otto-validation/train_parquet/035.parquet\n","/kaggle/input/otto-validation/train_parquet/057.parquet\n","/kaggle/input/otto-validation/train_parquet/018.parquet\n","/kaggle/input/otto-validation/train_parquet/063.parquet\n","/kaggle/input/otto-validation/train_parquet/000.parquet\n","/kaggle/input/otto-validation/train_parquet/091.parquet\n","/kaggle/input/otto-validation/train_parquet/001.parquet\n","/kaggle/input/otto-validation/train_parquet/090.parquet\n","/kaggle/input/otto-validation/train_parquet/038.parquet\n","/kaggle/input/otto-validation/train_parquet/093.parquet\n","/kaggle/input/otto-validation/train_parquet/070.parquet\n","/kaggle/input/otto-validation/train_parquet/059.parquet\n","/kaggle/input/otto-validation/train_parquet/062.parquet\n","/kaggle/input/otto-validation/train_parquet/008.parquet\n","/kaggle/input/otto-validation/train_parquet/098.parquet\n","/kaggle/input/otto-validation/train_parquet/071.parquet\n","/kaggle/input/otto-validation/train_parquet/026.parquet\n","/kaggle/input/otto-validation/train_parquet/010.parquet\n","/kaggle/input/otto-validation/train_parquet/030.parquet\n","/kaggle/input/otto-validation/train_parquet/092.parquet\n","/kaggle/input/otto-validation/train_parquet/002.parquet\n","/kaggle/input/otto-validation/train_parquet/014.parquet\n","/kaggle/input/otto-validation/train_parquet/085.parquet\n","/kaggle/input/otto-validation/train_parquet/009.parquet\n","/kaggle/input/otto-validation/train_parquet/003.parquet\n","/kaggle/input/otto-validation/train_parquet/067.parquet\n","/kaggle/input/otto-validation/train_parquet/054.parquet\n","/kaggle/input/otto-validation/train_parquet/004.parquet\n","/kaggle/input/otto-validation/train_parquet/053.parquet\n","/kaggle/input/otto-validation/train_parquet/073.parquet\n","/kaggle/input/otto-validation/train_parquet/061.parquet\n","/kaggle/input/otto-validation/train_parquet/088.parquet\n","/kaggle/input/otto-validation/train_parquet/075.parquet\n","/kaggle/input/otto-validation/train_parquet/042.parquet\n","/kaggle/input/otto-validation/train_parquet/086.parquet\n","/kaggle/input/otto-validation/train_parquet/025.parquet\n","/kaggle/input/otto-validation/train_parquet/048.parquet\n","/kaggle/input/otto-validation/train_parquet/039.parquet\n","/kaggle/input/otto-validation/train_parquet/031.parquet\n","/kaggle/input/otto-validation/train_parquet/072.parquet\n","/kaggle/input/otto-validation/train_parquet/094.parquet\n","/kaggle/input/otto-validation/train_parquet/013.parquet\n","/kaggle/input/otto-validation/train_parquet/069.parquet\n","/kaggle/input/otto-validation/train_parquet/064.parquet\n","/kaggle/input/otto-validation/train_parquet/079.parquet\n","/kaggle/input/otto-validation/train_parquet/083.parquet\n","/kaggle/input/otto-validation/train_parquet/044.parquet\n","/kaggle/input/otto-validation/train_parquet/080.parquet\n","/kaggle/input/otto-validation/train_parquet/076.parquet\n","/kaggle/input/otto-recommender-system/sample_submission.csv\n","/kaggle/input/otto-recommender-system/test.jsonl\n","/kaggle/input/otto-recommender-system/train.jsonl\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Candidate ReRank model\n","- feature engineering\n","- merging them into items and users\n","- training a reranker model\n","- choose final top-20"]},{"cell_type":"markdown","metadata":{},"source":["# step1 - Candidate Generation with RAPIDS"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:09:48.332012Z","iopub.status.busy":"2022-12-22T14:09:48.331361Z","iopub.status.idle":"2022-12-22T14:09:48.343283Z","shell.execute_reply":"2022-12-22T14:09:48.342372Z","shell.execute_reply.started":"2022-12-22T14:09:48.331974Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["We will use RAPIDS version 21.10.01\n"]}],"source":["VER = 6\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import os, sys, pickle, glob, gc\n","from collections import Counter\n","import cudf, itertools\n","print('We will use RAPIDS version',cudf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Three Co-visitation Matrices with RAPIDS\n","- cuDF on GPU가 pandas CPU보다 30배 빠르다고 한다.\n","- 아래 데이터는 저자가 parquet 파일로 원본데이터를 쪼갠 것을 이용한 것이다."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:09:48.345607Z","iopub.status.busy":"2022-12-22T14:09:48.344951Z","iopub.status.idle":"2022-12-22T14:10:32.367114Z","shell.execute_reply":"2022-12-22T14:10:32.365939Z","shell.execute_reply.started":"2022-12-22T14:09:48.345572Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["We will process 120 files, in groups of 5 and chunks of 20.\n","CPU times: user 33.3 s, sys: 5.06 s, total: 38.4 s\n","Wall time: 44 s\n"]}],"source":["%%time\n","# CACHE FUNCTIONS\n","def read_file(f):\n","    return cudf.DataFrame(data_cache[f])\n","\n","def read_file_to_cache(f):\n","    df = pd.read_parquet(f)\n","    df.ts = (df.ts/1000).astype('int32')\n","    df['type'] = df['type'].map(type_labels).astype('int8')\n","    return df\n","\n","# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n","data_cache = {}\n","type_labels = {'clicks':0, 'carts':1, 'orders':2}\n","files = glob.glob('../input/otto-validation/*_parquet/*')\n","\n","for f in files: data_cache[f] = read_file_to_cache(f)\n","\n","# CHUNK PARAMETERS\n","READ_CT = 5\n","CHUNK = int( np.ceil( len(files)/6 ))\n","print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Co-visitation Matrix - Type Weighted"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:10:32.370921Z","iopub.status.busy":"2022-12-22T14:10:32.370287Z","iopub.status.idle":"2022-12-22T14:12:46.015547Z","shell.execute_reply":"2022-12-22T14:12:46.014253Z","shell.execute_reply.started":"2022-12-22T14:10:32.370881Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### DISK PART 1\n","Processing files 0 thru 19 in groups of 5...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n","  \"When using a sequence of booleans for `ascending`, \"\n"]},{"name":"stdout","output_type":"stream","text":["0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 2\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 3\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 4\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","CPU times: user 1min 25s, sys: 46 s, total: 2min 11s\n","Wall time: 2min 13s\n"]}],"source":["%%time\n","type_weight = {0:1, 1:6, 2:3}\n","\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 4\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # => OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # => INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            \n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i<b:\n","                    df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            \n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n<30].drop('n',axis=1)\n","            \n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n","            \n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n","            \n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n","            df['wgt'] = df.type_y.map(type_weight)\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            \n","            # COMBINE INNER CHUNKS\n","            if k==a:\n","                tmp2 = df\n","            else:\n","                tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","        print()\n","        \n","        # COMBINE OUTER CHUNKS\n","        if a==0:\n","            tmp = tmp2\n","        else:\n","            tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","        \n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    \n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n","    \n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Buy-Buy Co-visitation Matrix"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:12:46.023031Z","iopub.status.busy":"2022-12-22T14:12:46.022073Z","iopub.status.idle":"2022-12-22T14:13:05.781270Z","shell.execute_reply":"2022-12-22T14:13:05.780106Z","shell.execute_reply.started":"2022-12-22T14:12:46.022991Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### DISK PART 1\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , "]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n","  \"When using a sequence of booleans for `ascending`, \"\n"]},{"name":"stdout","output_type":"stream","text":["10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","CPU times: user 13.7 s, sys: 5.57 s, total: 19.2 s\n","Wall time: 19.7 s\n"]}],"source":["%%time\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 1\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # => OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # => INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i < b:\n","                    df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            \n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n<30].drop('n',axis=1)\n","            \n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n","            \n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n","            \n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n","            df['wgt'] = 1\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            \n","            # COMBINE INNER CHUNKS\n","            if k==a:\n","                tmp2 = df\n","            else:\n","                tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","        print()\n","        \n","        # COMBINE OUTER CHUNKS\n","        if a==0:\n","            tmp = tmp2\n","        else:\n","            tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","        \n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    \n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n","    \n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Clicks Co-visitation matrix - Time Weighted"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:13:05.784134Z","iopub.status.busy":"2022-12-22T14:13:05.783394Z","iopub.status.idle":"2022-12-22T14:15:16.073744Z","shell.execute_reply":"2022-12-22T14:15:16.072619Z","shell.execute_reply.started":"2022-12-22T14:13:05.784093Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### DISK PART 1\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 2\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 3\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","\n","### DISK PART 4\n","Processing files 0 thru 19 in groups of 5...\n","0 , 5 , 10 , 15 , \n","Processing files 20 thru 39 in groups of 5...\n","20 , 25 , 30 , 35 , \n","Processing files 40 thru 59 in groups of 5...\n","40 , 45 , 50 , 55 , \n","Processing files 60 thru 79 in groups of 5...\n","60 , 65 , 70 , 75 , \n","Processing files 80 thru 99 in groups of 5...\n","80 , 85 , 90 , 95 , \n","Processing files 100 thru 119 in groups of 5...\n","100 , 105 , 110 , 115 , \n","CPU times: user 1min 25s, sys: 44.4 s, total: 2min 9s\n","Wall time: 2min 10s\n"]}],"source":["%%time\n","# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n","DISK_PIECES = 4\n","SIZE = 1.86e6/DISK_PIECES\n","\n","# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n","for PART in range(DISK_PIECES):\n","    print()\n","    print('### DISK PART',PART+1)\n","    \n","    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n","    # => OUTER CHUNKS\n","    for j in range(6):\n","        a = j*CHUNK\n","        b = min( (j+1)*CHUNK, len(files) )\n","        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n","        \n","        # => INNER CHUNKS\n","        for k in range(a,b,READ_CT):\n","            # READ FILE\n","            df = [read_file(files[k])]\n","            for i in range(1,READ_CT): \n","                if k+i<b: df.append( read_file(files[k+i]) )\n","            df = cudf.concat(df,ignore_index=True,axis=0)\n","            df = df.sort_values(['session','ts'],ascending=[True,False])\n","            # USE TAIL OF SESSION\n","            df = df.reset_index(drop=True)\n","            df['n'] = df.groupby('session').cumcount()\n","            df = df.loc[df.n<30].drop('n',axis=1)\n","            # CREATE PAIRS\n","            df = df.merge(df,on='session')\n","            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n","            # MEMORY MANAGEMENT COMPUTE IN PARTS\n","            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n","            # ASSIGN WEIGHTS\n","            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n","            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n","            df = df[['aid_x','aid_y','wgt']]\n","            df.wgt = df.wgt.astype('float32')\n","            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n","            # COMBINE INNER CHUNKS\n","            if k==a: tmp2 = df\n","            else: tmp2 = tmp2.add(df, fill_value=0)\n","            print(k,', ',end='')\n","        print()\n","        # COMBINE OUTER CHUNKS\n","        if a==0: tmp = tmp2\n","        else: tmp = tmp.add(tmp2, fill_value=0)\n","        del tmp2, df\n","        gc.collect()\n","    # CONVERT MATRIX TO DICTIONARY\n","    tmp = tmp.reset_index()\n","    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n","    # SAVE TOP 40\n","    tmp = tmp.reset_index(drop=True)\n","    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n","    tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n","    # SAVE PART TO DISK (convert to pandas first uses less memory)\n","    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:15:16.075666Z","iopub.status.busy":"2022-12-22T14:15:16.075215Z","iopub.status.idle":"2022-12-22T14:15:16.233952Z","shell.execute_reply":"2022-12-22T14:15:16.232648Z","shell.execute_reply.started":"2022-12-22T14:15:16.075628Z"},"trusted":true},"outputs":[],"source":["# FREE MEMORY\n","del data_cache, tmp\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## step2 - ReRank"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:15:16.236327Z","iopub.status.busy":"2022-12-22T14:15:16.235391Z","iopub.status.idle":"2022-12-22T14:15:17.682357Z","shell.execute_reply":"2022-12-22T14:15:17.681268Z","shell.execute_reply.started":"2022-12-22T14:15:16.236287Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test data has shape (7683577, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>session</th>\n","      <th>aid</th>\n","      <th>ts</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12089221</td>\n","      <td>700554</td>\n","      <td>1661448002</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12089221</td>\n","      <td>619488</td>\n","      <td>1661448024</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12089221</td>\n","      <td>579241</td>\n","      <td>1661449547</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12089221</td>\n","      <td>619488</td>\n","      <td>1661449585</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12089221</td>\n","      <td>619488</td>\n","      <td>1661456661</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    session     aid          ts  type\n","0  12089221  700554  1661448002     0\n","1  12089221  619488  1661448024     0\n","2  12089221  579241  1661449547     0\n","3  12089221  619488  1661449585     0\n","4  12089221  619488  1661456661     0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def load_test():    \n","    dfs = []\n","    for e, chunk_file in enumerate(glob.glob('../input/otto-validation/test_parquet/*')):\n","        chunk = pd.read_parquet(chunk_file)\n","        chunk.ts = (chunk.ts/1000).astype('int32')\n","        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n","        dfs.append(chunk)\n","    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n","\n","test_df = load_test()\n","print('Test data has shape',test_df.shape)\n","test_df.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:15:17.684362Z","iopub.status.busy":"2022-12-22T14:15:17.683989Z","iopub.status.idle":"2022-12-22T14:16:54.485516Z","shell.execute_reply":"2022-12-22T14:16:54.484549Z","shell.execute_reply.started":"2022-12-22T14:15:17.684326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Here are size of our 3 co-visitation matrices:\n","1812132 1055146 1812132\n","CPU times: user 1min 34s, sys: 4.02 s, total: 1min 38s\n","Wall time: 1min 36s\n"]}],"source":["%%time\n","# LOAD THREE CO-VISITATION MATRICES\n","def pqt_to_dict(df):\n","    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n","\n","top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n","for k in range(1,DISK_PIECES): \n","    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n","    \n","top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n","for k in range(1,DISK_PIECES): \n","    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n","    \n","top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n","\n","# TOP CLICKS AND ORDERS IN TEST\n","top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n","top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n","\n","print('Here are size of our 3 co-visitation matrices:')\n","print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:16:54.492068Z","iopub.status.busy":"2022-12-22T14:16:54.491345Z","iopub.status.idle":"2022-12-22T14:16:54.541477Z","shell.execute_reply":"2022-12-22T14:16:54.540350Z","shell.execute_reply.started":"2022-12-22T14:16:54.492024Z"},"trusted":true},"outputs":[],"source":["#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","\n","def suggest_clicks(df):\n","    # USE USER HISTORY AIDS AND TYPES\n","    aids=df.aid.tolist()\n","    types = df.type.tolist()\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    # RERANK CANDIDATES USING WEIGHTS\n","    if len(unique_aids) >= 20:\n","        weights = np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        for aid, w, t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n","        return sorted_aids\n","    # USE \"CLICKS\" CO-VISITATION MATRIX\n","    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n","    # RERANK CANDIDATES\n","    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n","    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n","    # USE TOP20 TEST CLICKS\n","    return result + list(top_clicks)[:20-len(result)]\n","\n","def suggest_buys(df):\n","    # USE USER HISTORY AIDS AND TYPES\n","    aids=df.aid.tolist()\n","    types = df.type.tolist()\n","    # UNIQUE AIDS AND UNIQUE BUYS\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    df = df.loc[(df['type']==1)|(df['type']==2)]\n","    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n","    # RERANK CANDIDATES USING WEIGHTS\n","    if len(unique_aids)>=20:\n","        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n","        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","        for aid in aids3: aids_temp[aid] += 0.1\n","        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n","        return sorted_aids\n","    # USE \"CART ORDER\" CO-VISITATION MATRIX\n","    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n","    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n","    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","    # RERANK CANDIDATES\n","    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n","    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n","    # USE TOP20 TEST ORDERS\n","    return result + list(top_orders)[:20-len(result)]"]},{"cell_type":"markdown","metadata":{},"source":["# Create Submission CSV"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:16:54.544086Z","iopub.status.busy":"2022-12-22T14:16:54.543299Z","iopub.status.idle":"2022-12-22T14:44:53.158062Z","shell.execute_reply":"2022-12-22T14:44:53.157099Z","shell.execute_reply.started":"2022-12-22T14:16:54.544048Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 27min 44s, sys: 9.76 s, total: 27min 54s\n","Wall time: 27min 58s\n"]}],"source":["%%time\n","pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n","    lambda x: suggest_clicks(x)\n",")\n","\n","pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n","    lambda x: suggest_buys(x)\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:44:53.160096Z","iopub.status.busy":"2022-12-22T14:44:53.159701Z","iopub.status.idle":"2022-12-22T14:44:56.613063Z","shell.execute_reply":"2022-12-22T14:44:56.612018Z","shell.execute_reply.started":"2022-12-22T14:44:53.160058Z"},"trusted":true},"outputs":[],"source":["clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n","orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n","carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:44:56.615037Z","iopub.status.busy":"2022-12-22T14:44:56.614341Z","iopub.status.idle":"2022-12-22T14:45:38.195211Z","shell.execute_reply":"2022-12-22T14:45:38.194122Z","shell.execute_reply.started":"2022-12-22T14:44:56.614992Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>session_type</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11098528_clicks</td>\n","      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11098529_clicks</td>\n","      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11098530_clicks</td>\n","      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11098531_clicks</td>\n","      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11098532_clicks</td>\n","      <td>876469 7651 108125 1202618 1159379 77906 17040...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      session_type                                             labels\n","0  11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...\n","1  11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...\n","2  11098530_clicks  409236 264500 1603001 963957 254154 583026 167...\n","3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n","4  11098532_clicks  876469 7651 108125 1202618 1159379 77906 17040..."]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n","pred_df.columns = [\"session_type\", \"labels\"]\n","pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n","pred_df.to_csv(\"validation_preds.csv\", index=False)\n","pred_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Compute Vaildation Score"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:45:38.197733Z","iopub.status.busy":"2022-12-22T14:45:38.196750Z","iopub.status.idle":"2022-12-22T14:45:41.247916Z","shell.execute_reply":"2022-12-22T14:45:41.246920Z","shell.execute_reply.started":"2022-12-22T14:45:38.197695Z"},"trusted":true},"outputs":[],"source":["# FREE MEMORY\n","del pred_df_clicks, pred_df_buys, clicks_pred_df, orders_pred_df, carts_pred_df\n","del top_20_clicks, top_20_buy2buy, top_20_buys, top_clicks, top_orders, test_df\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-22T14:45:41.249936Z","iopub.status.busy":"2022-12-22T14:45:41.249557Z","iopub.status.idle":"2022-12-22T14:47:31.890874Z","shell.execute_reply":"2022-12-22T14:47:31.889726Z","shell.execute_reply.started":"2022-12-22T14:45:41.249897Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["clicks recall = 0.5255597442145808\n","carts recall = 0.4093761817368105\n","orders recall = 0.6489309071410104\n","=============\n","Overall Recall = 0.5647273732271075\n","=============\n","CPU times: user 1min 48s, sys: 2.2 s, total: 1min 50s\n","Wall time: 1min 50s\n"]}],"source":["%%time\n","# COMPUTE METRIC\n","score = 0\n","weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n","for t in ['clicks','carts','orders']:\n","    sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n","    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n","    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n","    test_labels = pd.read_parquet('../input/otto-validation/test_labels.parquet')\n","    test_labels = test_labels.loc[test_labels['type']==t]\n","    test_labels = test_labels.merge(sub, how='left', on=['session'])\n","    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n","    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n","    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n","    score += weights[t]*recall\n","    print(f'{t} recall =',recall)\n","    \n","print('=============')\n","print('Overall Recall =',score)\n","print('=============')"]}],"metadata":{"kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, May 24 2022, 22:40:11) \n[GCC 9.4.0]"},"vscode":{"interpreter":{"hash":"8c68d4e82246607c407fe48e8ea8c83604bff69a3864f23665571bb2eafd238d"}}},"nbformat":4,"nbformat_minor":4}
