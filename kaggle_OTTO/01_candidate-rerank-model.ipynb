{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-22T14:09:48.287368Z","iopub.execute_input":"2022-12-22T14:09:48.287926Z","iopub.status.idle":"2022-12-22T14:09:48.328186Z","shell.execute_reply.started":"2022-12-22T14:09:48.287815Z","shell.execute_reply":"2022-12-22T14:09:48.327275Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/otto-validation/test_labels.parquet\n/kaggle/input/otto-validation/test_parquet/011.parquet\n/kaggle/input/otto-validation/test_parquet/006.parquet\n/kaggle/input/otto-validation/test_parquet/017.parquet\n/kaggle/input/otto-validation/test_parquet/016.parquet\n/kaggle/input/otto-validation/test_parquet/005.parquet\n/kaggle/input/otto-validation/test_parquet/019.parquet\n/kaggle/input/otto-validation/test_parquet/012.parquet\n/kaggle/input/otto-validation/test_parquet/015.parquet\n/kaggle/input/otto-validation/test_parquet/007.parquet\n/kaggle/input/otto-validation/test_parquet/018.parquet\n/kaggle/input/otto-validation/test_parquet/000.parquet\n/kaggle/input/otto-validation/test_parquet/001.parquet\n/kaggle/input/otto-validation/test_parquet/008.parquet\n/kaggle/input/otto-validation/test_parquet/010.parquet\n/kaggle/input/otto-validation/test_parquet/002.parquet\n/kaggle/input/otto-validation/test_parquet/014.parquet\n/kaggle/input/otto-validation/test_parquet/009.parquet\n/kaggle/input/otto-validation/test_parquet/003.parquet\n/kaggle/input/otto-validation/test_parquet/004.parquet\n/kaggle/input/otto-validation/test_parquet/013.parquet\n/kaggle/input/otto-validation/train_parquet/045.parquet\n/kaggle/input/otto-validation/train_parquet/011.parquet\n/kaggle/input/otto-validation/train_parquet/050.parquet\n/kaggle/input/otto-validation/train_parquet/087.parquet\n/kaggle/input/otto-validation/train_parquet/040.parquet\n/kaggle/input/otto-validation/train_parquet/049.parquet\n/kaggle/input/otto-validation/train_parquet/006.parquet\n/kaggle/input/otto-validation/train_parquet/017.parquet\n/kaggle/input/otto-validation/train_parquet/024.parquet\n/kaggle/input/otto-validation/train_parquet/032.parquet\n/kaggle/input/otto-validation/train_parquet/078.parquet\n/kaggle/input/otto-validation/train_parquet/089.parquet\n/kaggle/input/otto-validation/train_parquet/023.parquet\n/kaggle/input/otto-validation/train_parquet/029.parquet\n/kaggle/input/otto-validation/train_parquet/068.parquet\n/kaggle/input/otto-validation/train_parquet/074.parquet\n/kaggle/input/otto-validation/train_parquet/096.parquet\n/kaggle/input/otto-validation/train_parquet/016.parquet\n/kaggle/input/otto-validation/train_parquet/077.parquet\n/kaggle/input/otto-validation/train_parquet/021.parquet\n/kaggle/input/otto-validation/train_parquet/046.parquet\n/kaggle/input/otto-validation/train_parquet/055.parquet\n/kaggle/input/otto-validation/train_parquet/058.parquet\n/kaggle/input/otto-validation/train_parquet/037.parquet\n/kaggle/input/otto-validation/train_parquet/051.parquet\n/kaggle/input/otto-validation/train_parquet/005.parquet\n/kaggle/input/otto-validation/train_parquet/019.parquet\n/kaggle/input/otto-validation/train_parquet/081.parquet\n/kaggle/input/otto-validation/train_parquet/028.parquet\n/kaggle/input/otto-validation/train_parquet/097.parquet\n/kaggle/input/otto-validation/train_parquet/095.parquet\n/kaggle/input/otto-validation/train_parquet/012.parquet\n/kaggle/input/otto-validation/train_parquet/027.parquet\n/kaggle/input/otto-validation/train_parquet/033.parquet\n/kaggle/input/otto-validation/train_parquet/052.parquet\n/kaggle/input/otto-validation/train_parquet/020.parquet\n/kaggle/input/otto-validation/train_parquet/065.parquet\n/kaggle/input/otto-validation/train_parquet/036.parquet\n/kaggle/input/otto-validation/train_parquet/056.parquet\n/kaggle/input/otto-validation/train_parquet/043.parquet\n/kaggle/input/otto-validation/train_parquet/060.parquet\n/kaggle/input/otto-validation/train_parquet/015.parquet\n/kaggle/input/otto-validation/train_parquet/066.parquet\n/kaggle/input/otto-validation/train_parquet/034.parquet\n/kaggle/input/otto-validation/train_parquet/099.parquet\n/kaggle/input/otto-validation/train_parquet/047.parquet\n/kaggle/input/otto-validation/train_parquet/041.parquet\n/kaggle/input/otto-validation/train_parquet/022.parquet\n/kaggle/input/otto-validation/train_parquet/007.parquet\n/kaggle/input/otto-validation/train_parquet/082.parquet\n/kaggle/input/otto-validation/train_parquet/084.parquet\n/kaggle/input/otto-validation/train_parquet/035.parquet\n/kaggle/input/otto-validation/train_parquet/057.parquet\n/kaggle/input/otto-validation/train_parquet/018.parquet\n/kaggle/input/otto-validation/train_parquet/063.parquet\n/kaggle/input/otto-validation/train_parquet/000.parquet\n/kaggle/input/otto-validation/train_parquet/091.parquet\n/kaggle/input/otto-validation/train_parquet/001.parquet\n/kaggle/input/otto-validation/train_parquet/090.parquet\n/kaggle/input/otto-validation/train_parquet/038.parquet\n/kaggle/input/otto-validation/train_parquet/093.parquet\n/kaggle/input/otto-validation/train_parquet/070.parquet\n/kaggle/input/otto-validation/train_parquet/059.parquet\n/kaggle/input/otto-validation/train_parquet/062.parquet\n/kaggle/input/otto-validation/train_parquet/008.parquet\n/kaggle/input/otto-validation/train_parquet/098.parquet\n/kaggle/input/otto-validation/train_parquet/071.parquet\n/kaggle/input/otto-validation/train_parquet/026.parquet\n/kaggle/input/otto-validation/train_parquet/010.parquet\n/kaggle/input/otto-validation/train_parquet/030.parquet\n/kaggle/input/otto-validation/train_parquet/092.parquet\n/kaggle/input/otto-validation/train_parquet/002.parquet\n/kaggle/input/otto-validation/train_parquet/014.parquet\n/kaggle/input/otto-validation/train_parquet/085.parquet\n/kaggle/input/otto-validation/train_parquet/009.parquet\n/kaggle/input/otto-validation/train_parquet/003.parquet\n/kaggle/input/otto-validation/train_parquet/067.parquet\n/kaggle/input/otto-validation/train_parquet/054.parquet\n/kaggle/input/otto-validation/train_parquet/004.parquet\n/kaggle/input/otto-validation/train_parquet/053.parquet\n/kaggle/input/otto-validation/train_parquet/073.parquet\n/kaggle/input/otto-validation/train_parquet/061.parquet\n/kaggle/input/otto-validation/train_parquet/088.parquet\n/kaggle/input/otto-validation/train_parquet/075.parquet\n/kaggle/input/otto-validation/train_parquet/042.parquet\n/kaggle/input/otto-validation/train_parquet/086.parquet\n/kaggle/input/otto-validation/train_parquet/025.parquet\n/kaggle/input/otto-validation/train_parquet/048.parquet\n/kaggle/input/otto-validation/train_parquet/039.parquet\n/kaggle/input/otto-validation/train_parquet/031.parquet\n/kaggle/input/otto-validation/train_parquet/072.parquet\n/kaggle/input/otto-validation/train_parquet/094.parquet\n/kaggle/input/otto-validation/train_parquet/013.parquet\n/kaggle/input/otto-validation/train_parquet/069.parquet\n/kaggle/input/otto-validation/train_parquet/064.parquet\n/kaggle/input/otto-validation/train_parquet/079.parquet\n/kaggle/input/otto-validation/train_parquet/083.parquet\n/kaggle/input/otto-validation/train_parquet/044.parquet\n/kaggle/input/otto-validation/train_parquet/080.parquet\n/kaggle/input/otto-validation/train_parquet/076.parquet\n/kaggle/input/otto-recommender-system/sample_submission.csv\n/kaggle/input/otto-recommender-system/test.jsonl\n/kaggle/input/otto-recommender-system/train.jsonl\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Candidate ReRank model\n- feature engineering\n- merging them into items and users\n- training a reranker model\n- choose final top-20","metadata":{}},{"cell_type":"markdown","source":"# step1 - Candidate Generation with RAPIDS","metadata":{}},{"cell_type":"code","source":"VER = 6\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:09:48.331361Z","iopub.execute_input":"2022-12-22T14:09:48.332012Z","iopub.status.idle":"2022-12-22T14:09:48.343283Z","shell.execute_reply.started":"2022-12-22T14:09:48.331974Z","shell.execute_reply":"2022-12-22T14:09:48.342372Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"We will use RAPIDS version 21.10.01\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Compute Three Co-visitation Matrices with RAPIDS\n- cuDF on GPU가 pandas CPU보다 30배 빠르다고 한다.\n- 아래 데이터는 저자가 parquet 파일로 원본데이터를 쪼갠 것을 이용한 것이다.","metadata":{}},{"cell_type":"code","source":"%%time\n# CACHE FUNCTIONS\ndef read_file(f):\n    return cudf.DataFrame(data_cache[f])\n\ndef read_file_to_cache(f):\n    df = pd.read_parquet(f)\n    df.ts = (df.ts/1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    return df\n\n# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\ndata_cache = {}\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\nfiles = glob.glob('../input/otto-validation/*_parquet/*')\n\nfor f in files: data_cache[f] = read_file_to_cache(f)\n\n# CHUNK PARAMETERS\nREAD_CT = 5\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:09:48.344951Z","iopub.execute_input":"2022-12-22T14:09:48.345607Z","iopub.status.idle":"2022-12-22T14:10:32.367114Z","shell.execute_reply.started":"2022-12-22T14:09:48.345572Z","shell.execute_reply":"2022-12-22T14:10:32.365939Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"We will process 120 files, in groups of 5 and chunks of 20.\nCPU times: user 33.3 s, sys: 5.06 s, total: 38.4 s\nWall time: 44 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1. Co-visitation Matrix - Type Weighted","metadata":{}},{"cell_type":"code","source":"%%time\ntype_weight = {0:1, 1:6, 2:3}\n\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            \n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b:\n                    df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a:\n                tmp2 = df\n            else:\n                tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0:\n            tmp = tmp2\n        else:\n            tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n        \n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:10:32.370287Z","iopub.execute_input":"2022-12-22T14:10:32.370921Z","iopub.status.idle":"2022-12-22T14:12:46.015547Z","shell.execute_reply.started":"2022-12-22T14:10:32.370881Z","shell.execute_reply":"2022-12-22T14:12:46.014253Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 2\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 3\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 4\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \nCPU times: user 1min 25s, sys: 46 s, total: 2min 11s\nWall time: 2min 13s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Buy-Buy Co-visitation Matrix","metadata":{}},{"cell_type":"code","source":"%%time\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 1\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i < b:\n                    df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a:\n                tmp2 = df\n            else:\n                tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0:\n            tmp = tmp2\n        else:\n            tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n        \n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:12:46.022073Z","iopub.execute_input":"2022-12-22T14:12:46.023031Z","iopub.status.idle":"2022-12-22T14:13:05.781270Z","shell.execute_reply.started":"2022-12-22T14:12:46.022991Z","shell.execute_reply":"2022-12-22T14:13:05.780106Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \nCPU times: user 13.7 s, sys: 5.57 s, total: 19.2 s\nWall time: 19.7 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3. Clicks Co-visitation matrix - Time Weighted","metadata":{}},{"cell_type":"code","source":"%%time\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:13:05.783394Z","iopub.execute_input":"2022-12-22T14:13:05.784134Z","iopub.status.idle":"2022-12-22T14:15:16.073744Z","shell.execute_reply.started":"2022-12-22T14:13:05.784093Z","shell.execute_reply":"2022-12-22T14:15:16.072619Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 2\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 3\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \n\n### DISK PART 4\nProcessing files 0 thru 19 in groups of 5...\n0 , 5 , 10 , 15 , \nProcessing files 20 thru 39 in groups of 5...\n20 , 25 , 30 , 35 , \nProcessing files 40 thru 59 in groups of 5...\n40 , 45 , 50 , 55 , \nProcessing files 60 thru 79 in groups of 5...\n60 , 65 , 70 , 75 , \nProcessing files 80 thru 99 in groups of 5...\n80 , 85 , 90 , 95 , \nProcessing files 100 thru 119 in groups of 5...\n100 , 105 , 110 , 115 , \nCPU times: user 1min 25s, sys: 44.4 s, total: 2min 9s\nWall time: 2min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"# FREE MEMORY\ndel data_cache, tmp\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:15:16.075215Z","iopub.execute_input":"2022-12-22T14:15:16.075666Z","iopub.status.idle":"2022-12-22T14:15:16.233952Z","shell.execute_reply.started":"2022-12-22T14:15:16.075628Z","shell.execute_reply":"2022-12-22T14:15:16.232648Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## step2 - ReRank","metadata":{}},{"cell_type":"code","source":"def load_test():    \n    dfs = []\n    for e, chunk_file in enumerate(glob.glob('../input/otto-validation/test_parquet/*')):\n        chunk = pd.read_parquet(chunk_file)\n        chunk.ts = (chunk.ts/1000).astype('int32')\n        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n        dfs.append(chunk)\n    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n\ntest_df = load_test()\nprint('Test data has shape',test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:15:16.235391Z","iopub.execute_input":"2022-12-22T14:15:16.236327Z","iopub.status.idle":"2022-12-22T14:15:17.682357Z","shell.execute_reply.started":"2022-12-22T14:15:16.236287Z","shell.execute_reply":"2022-12-22T14:15:17.681268Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Test data has shape (7683577, 4)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    session     aid          ts  type\n0  12089221  700554  1661448002     0\n1  12089221  619488  1661448024     0\n2  12089221  579241  1661449547     0\n3  12089221  619488  1661449585     0\n4  12089221  619488  1661456661     0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session</th>\n      <th>aid</th>\n      <th>ts</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12089221</td>\n      <td>700554</td>\n      <td>1661448002</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12089221</td>\n      <td>619488</td>\n      <td>1661448024</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12089221</td>\n      <td>579241</td>\n      <td>1661449547</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12089221</td>\n      <td>619488</td>\n      <td>1661449585</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12089221</td>\n      <td>619488</td>\n      <td>1661456661</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# LOAD THREE CO-VISITATION MATRICES\ndef pqt_to_dict(df):\n    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n\ntop_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\nfor k in range(1,DISK_PIECES): \n    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n    \ntop_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\nfor k in range(1,DISK_PIECES): \n    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n    \ntop_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n\n# TOP CLICKS AND ORDERS IN TEST\ntop_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\ntop_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n\nprint('Here are size of our 3 co-visitation matrices:')\nprint( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:15:17.683989Z","iopub.execute_input":"2022-12-22T14:15:17.684362Z","iopub.status.idle":"2022-12-22T14:16:54.485516Z","shell.execute_reply.started":"2022-12-22T14:15:17.684326Z","shell.execute_reply":"2022-12-22T14:16:54.484549Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Here are size of our 3 co-visitation matrices:\n1812132 1055146 1812132\nCPU times: user 1min 34s, sys: 4.02 s, total: 1min 38s\nWall time: 1min 36s\n","output_type":"stream"}]},{"cell_type":"code","source":"#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\ntype_weight_multipliers = {0: 1, 1: 6, 2: 3}\n\ndef suggest_clicks(df):\n    # USE USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids) >= 20:\n        weights = np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid, w, t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CLICKS\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST CLICKS\n    return result + list(top_clicks)[:20-len(result)]\n\ndef suggest_buys(df):\n    # USE USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    # UNIQUE AIDS AND UNIQUE BUYS\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    df = df.loc[(df['type']==1)|(df['type']==2)]\n    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids)>=20:\n        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n        for aid in aids3: aids_temp[aid] += 0.1\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CART ORDER\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST ORDERS\n    return result + list(top_orders)[:20-len(result)]","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:16:54.491345Z","iopub.execute_input":"2022-12-22T14:16:54.492068Z","iopub.status.idle":"2022-12-22T14:16:54.541477Z","shell.execute_reply.started":"2022-12-22T14:16:54.492024Z","shell.execute_reply":"2022-12-22T14:16:54.540350Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission CSV","metadata":{}},{"cell_type":"code","source":"%%time\npred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_clicks(x)\n)\n\npred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_buys(x)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:16:54.543299Z","iopub.execute_input":"2022-12-22T14:16:54.544086Z","iopub.status.idle":"2022-12-22T14:44:53.158062Z","shell.execute_reply.started":"2022-12-22T14:16:54.544048Z","shell.execute_reply":"2022-12-22T14:44:53.157099Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CPU times: user 27min 44s, sys: 9.76 s, total: 27min 54s\nWall time: 27min 58s\n","output_type":"stream"}]},{"cell_type":"code","source":"clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\norders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\ncarts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:44:53.159701Z","iopub.execute_input":"2022-12-22T14:44:53.160096Z","iopub.status.idle":"2022-12-22T14:44:56.613063Z","shell.execute_reply.started":"2022-12-22T14:44:53.160058Z","shell.execute_reply":"2022-12-22T14:44:56.612018Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\npred_df.columns = [\"session_type\", \"labels\"]\npred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\npred_df.to_csv(\"validation_preds.csv\", index=False)\npred_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:44:56.614341Z","iopub.execute_input":"2022-12-22T14:44:56.615037Z","iopub.status.idle":"2022-12-22T14:45:38.195211Z","shell.execute_reply.started":"2022-12-22T14:44:56.614992Z","shell.execute_reply":"2022-12-22T14:45:38.194122Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      session_type                                             labels\n0  11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...\n1  11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...\n2  11098530_clicks  409236 264500 1603001 963957 254154 583026 167...\n3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n4  11098532_clicks  876469 7651 108125 1202618 1159379 77906 17040...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session_type</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11098528_clicks</td>\n      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11098529_clicks</td>\n      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11098530_clicks</td>\n      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11098531_clicks</td>\n      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11098532_clicks</td>\n      <td>876469 7651 108125 1202618 1159379 77906 17040...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Compute Vaildation Score","metadata":{}},{"cell_type":"code","source":"# FREE MEMORY\ndel pred_df_clicks, pred_df_buys, clicks_pred_df, orders_pred_df, carts_pred_df\ndel top_20_clicks, top_20_buy2buy, top_20_buys, top_clicks, top_orders, test_df\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:45:38.196750Z","iopub.execute_input":"2022-12-22T14:45:38.197733Z","iopub.status.idle":"2022-12-22T14:45:41.247916Z","shell.execute_reply.started":"2022-12-22T14:45:38.197695Z","shell.execute_reply":"2022-12-22T14:45:41.246920Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%%time\n# COMPUTE METRIC\nscore = 0\nweights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\nfor t in ['clicks','carts','orders']:\n    sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n    test_labels = pd.read_parquet('../input/otto-validation/test_labels.parquet')\n    test_labels = test_labels.loc[test_labels['type']==t]\n    test_labels = test_labels.merge(sub, how='left', on=['session'])\n    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n    score += weights[t]*recall\n    print(f'{t} recall =',recall)\n    \nprint('=============')\nprint('Overall Recall =',score)\nprint('=============')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T14:45:41.249557Z","iopub.execute_input":"2022-12-22T14:45:41.249936Z","iopub.status.idle":"2022-12-22T14:47:31.890874Z","shell.execute_reply.started":"2022-12-22T14:45:41.249897Z","shell.execute_reply":"2022-12-22T14:47:31.889726Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"clicks recall = 0.5255597442145808\ncarts recall = 0.4093761817368105\norders recall = 0.6489309071410104\n=============\nOverall Recall = 0.5647273732271075\n=============\nCPU times: user 1min 48s, sys: 2.2 s, total: 1min 50s\nWall time: 1min 50s\n","output_type":"stream"}]}]}